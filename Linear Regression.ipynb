{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "## 目的\n",
    "学习到一个映射关系 $f: X\\rightarrow y$, 即当给定一个新的待预测样本X时,我们可以通过学到的映射关系得到该样本的预测值y.\n",
    "## 前提\n",
    "当前,从目的的描述来看,要完成预测必须有一个前提,那就是输入X与输出y之间必须存在某种映射关系, 只是这种映射关系对我们来说是未知的,我们要通过学习来近似得到该映射关系\n",
    "## 表示\n",
    "假定X有n个特征, 我们将X到y的映射定义为如下形式, 令$x_0=1$以消去常数项,\n",
    "$$\n",
    "    \\begin{align}\n",
    "    h_\\theta(X) &= \\sum_{i=0}^n\\theta_ix_i = \\theta^TX \\\\\n",
    "     X&=\\begin{bmatrix}x_0\\\\ x_1\\\\ {\\vdots}\\\\ x_n\\end{bmatrix}  \\quad\n",
    "    \\theta = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ {\\vdots} \\\\ \\theta_n \\end{bmatrix}\n",
    "    \\end{align}\n",
    "$$\n",
    "$\\theta$为参数, $\\theta$确定了,那么这个映射也就确定了,因此,要学到一组$\\theta$即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练\n",
    "假设训练集中有m个训练样本:$D = \\{ X^{(i)}, y^{(i)} \\}_{i=1}^m$; 其中\n",
    "$$\n",
    "   X = \\begin{bmatrix}\n",
    "           x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & \\cdots & x_n^{(1)} \\\\ \n",
    "           x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & \\cdots & x_n^{(2)} \\\\ \n",
    "           \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "           x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & \\cdots & x_n^{(m)} \n",
    "         \\end{bmatrix} \\quad\n",
    "  y = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}\\end{bmatrix} \\quad\n",
    "  \\theta=\\begin{bmatrix}\\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_n \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "X的维度为$m\\times (n+1)$的矩阵;\n",
    "y的维度为$m\\times 1$的列向量;\n",
    "$\\theta$的维度为$(n+1) \\times 1$的列向量.\n",
    "\n",
    "该m个训练样本,以$\\theta$为参数时的预测值$\\hat{y}$ 为:\n",
    "$$\\hat{y} = h_{\\theta}(x) = x\\times\\theta$$\n",
    "\n",
    "结合我们的目标是学习一组$\\theta$使得其在训练数据集上得到的预测值与真实值越接近越好."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了衡量真实值与预测值的之间的差距,我们引入一个函数:损失函数(叫 cost function 或 loss function, 度量损失的函数):\n",
    "此处使用平方损失函数, 至于为什么用平方损失函数,解释见附录.\n",
    "$$\n",
    "    Loss(x^{(i)}) = (\\hat{y}^{(i)} - y^{(i)})^2 \n",
    "$$\n",
    "整个训练集的损失为:\n",
    "$$\n",
    "\\begin{align}\n",
    "    J(\\theta) &= \\frac{1}{2m}\\sum_i^m Loss(x^{(i)}) \\\\ \n",
    "    &= \\frac{1}{2m} \\sum_i^m ( \\hat{y}^{(i)} - y ^{(i)})^2 \\\\\n",
    "    &= \\frac{1}{2m} \\sum_i^m  (h_{\\theta}(x^{(i)}) - y^{(i)})^2  \\\\\n",
    "    &= \\frac{1}{2m} \\sum_i^m  (x^{(i)}\\theta - y^{(i)})^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "用向量表示上式为:\n",
    "$$\n",
    "    J(\\theta) = \\frac{1}{2m}(X\\theta - y)^T(X\\theta-y)\n",
    "$$\n",
    "\n",
    "最优值为\n",
    "$$\\theta^* = arg\\underset{\\theta}{min}J(\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化\n",
    "其实到这里,已经可以交给数学系的同学了, 观察函数$J(\\theta)$可知, 它是一个关于$\\theta$的二次函数, 也就是一个典型的凸函数, 最优化凸函数有专门的凸优化的理论支持, 有很多种方法可以优化这个函数.\n",
    "这里简单列举几个常用的优化方法;\n",
    "## 梯度下降\n",
    "### 代数方法\n",
    "分别对$\\theta$的每一个元素求导\n",
    "$$\n",
    "    J(\\theta) = \\frac{1}{2m} \\sum_i^m  (X^{(i)}\\times \\theta - y^{(i)})^2 = \\frac{1}{2m} \\sum_{i=1}^m( \\sum_{j=0}^nx_j^{(i)}\\theta_j - y^{(i)} )^2 \\\\\n",
    "$$\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial{\\theta_j}}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m \\left( (\\sum_{j=0}^nx_j^{(i)}\\theta_j-y^{(i)})x_j^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "然后依次更新$\\theta$\n",
    "$$\\theta_j^{t+1} = \\theta_j^t - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算梯度向量\n",
    "向量化的计算可以并行运算,并且很多包对矩阵向量运算增加了优化,因此多用向量化计算可以增加运算速度;\n",
    "这里要引入梯度的概念.\n",
    "#### 梯度\n",
    "$$\n",
    "  grad f(x_1, x_2, \\cdots, x_n)  = \\left ( \\frac{\\partial{f}}{\\partial{x_1}},  \\frac{\\partial{f}}{\\partial{x_2}},  \\cdots, \\frac{\\partial{f}}{\\partial{x_n}}  \\right)\n",
    "$$\n",
    "梯度表示了在向量空间某点处,函数沿着哪一个方向有最大的变化率.\n",
    "函数在某一点的梯度是这样一个向量,它的方向与取得最大方向导数的方向一致,而它的模为方向导数的最大值.\n",
    "\n",
    "根据梯度的定义,函数沿着梯度方向有最大变化率, 那么在这里需要优化损失的函数的时候,自然沿着负梯度方向是下降最快的方向.\n",
    "\n",
    "#### 计算\n",
    "残差$R = \\hat{y} - y = X\\theta - y$为m维列向量.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta) &= \\sum_{i=1}^m \\left( (\\sum_{j=0}^nx_j^{(i)}\\theta_j-y^{(i)})x_j^{(i)} \\right) \\\\\n",
    " & = R^T\\vec{x_j} \\\\\n",
    " 这里x_j为m维列向量 \n",
    " \\begin{bmatrix}\n",
    " x_j^1 \\\\\n",
    " x_j^2 \\\\\n",
    " \\vdots \\\\\n",
    " x_j^m \n",
    " \\end{bmatrix}\n",
    " \\end{align}\n",
    "$$\n",
    "\n",
    "因此这里J的梯度可以表示为\n",
    "$$ \n",
    "\\bigtriangledown_\\theta J(\\theta) = (R^T \\times X)^T = ((X\\theta-Y)^T X)^T\n",
    "$$\n",
    "\n",
    "$\\theta$更新规则为:\n",
    "$$\n",
    "\\theta=\\theta - \\alpha \\bigtriangledown_\\theta J(\\theta) = \\theta - \\alpha ((XW-Y)^T X)^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵求导法优化\n",
    "$$\n",
    "\\begin{align}\n",
    "J(\\theta) &= (X\\theta - y)^T(X\\theta-y) \\\\\n",
    "              &=(\\theta^TX^T -y^T)(X\\theta-y) \\\\\n",
    "              &=\\theta^TX^TX\\theta-y^TX\\theta - \\theta^TX^Ty + y^Ty \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "矩阵求导公式\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d(u^Tv)}{dx}&=\\frac{d(u^t)}{dx}\\cdot v + \\frac{d(v^T)}{dx}\\cdot u \\\\\n",
    "\\frac{dx^T}{dx}& = I \\\\\n",
    "\\frac{d(Ax)^T}{dx} & = A^T \\\\\n",
    "\\frac{dx}{dx^T}& = I \\\\\n",
    "\\frac{d(Ax)}{dx^T}& = A\n",
    "\\end{align}\n",
    "$$\n",
    "推论\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d(x^Tx)}{dx} = \\frac{d(x^T)}{dx}\\cdot x + \\frac{d(x^T)}{dx}\\cdot x = 2x \\\\\n",
    "\\frac{d(x^TAx)}{dx} = \\frac{d(x^T)}{dx}\\cdot Ax + \\frac{d(Ax)^T}{dx}\\cdot x = Ax+A^Tx\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "对$J(\\theta)$中的式子分别求导\n",
    "$$\n",
    "\\frac{\\partial{(\\theta^TX^TX\\theta)}}{\\partial{\\theta}} = X^TX\\theta+(X^TX)^T\\theta = 2X^TX\\theta\n",
    "$$\n",
    "其中$y^TX\\theta$为标量, $(1\\times m) \\times (m \\times n) \\times (n*1)$\n",
    "同理,$\\theta^TX^Ty$也为标量, 并且有$y^TX\\theta = \\theta^TX^Ty$, 因此为了方便计算导数,可以将$y^TX\\theta$化为$ \\theta^TX^Ty$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{(2\\theta^TX^Ty)}}{\\partial{\\theta}} = 2X^Ty\n",
    "$$\n",
    "\n",
    "得\n",
    "$$\n",
    "\\frac{\\partial{J(\\theta)}}{\\partial{\\theta}} = 2X^TX\\theta-2X^Ty\n",
    "$$\n",
    "令$\\frac{\\partial{J(\\theta)}}{\\partial{\\theta}} = 0$得\n",
    "$$\n",
    "\\begin{align}\n",
    "2X^TX\\theta - 2X^Ty = 0 \\\\\n",
    "\\Rightarrow\n",
    "X^TX\\theta = X^Ty \\\\\n",
    "\\Rightarrow\n",
    "\\theta = (X^TX)^{-1}X^Ty\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T09:33:36.350747Z",
     "start_time": "2018-11-12T09:33:36.113881Z"
    }
   },
   "source": [
    "## 其他梯度下降方法\n",
    "### BGD(批量梯度下降) \n",
    "即前文介绍方法,每次更新用到所有样本\n",
    "### SGD(随机梯度下降)\n",
    "每次更新只用到一个样本\n",
    "### 拟牛顿法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T01:52:28.038730Z",
     "start_time": "2018-11-17T01:52:26.910405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "from sklearn import datasets\n",
    "boston = datasets.load_boston() #Load and return the boston house-prices dataset\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T01:52:39.150033Z",
     "start_time": "2018-11-17T01:52:39.119342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13) (404,) (102, 13) (102,)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=4)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(type(X_train), type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T01:52:48.093682Z",
     "start_time": "2018-11-17T01:52:47.673193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10765981  0.04839376 -0.02320055  2.99304063 -2.16262312  5.95900965\n",
      " -0.01692421 -1.02725147  0.16689425 -0.0104551  -0.3752965   0.01426569\n",
      " -0.34632603]\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7020310444459503"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用sklearn提供的linear regression来训练数据\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "LR = LinearRegression(normalize=True,n_jobs=2, fit_intercept=False)\n",
    "\n",
    "LR.fit(X_train, y_train)\n",
    "print(LR.coef_)\n",
    "print(LR.intercept_)\n",
    "LR.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自己实现轮子\n",
    "### 最简单的线性回归\n",
    " 不带正则化,不带标准化.\n",
    " BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T01:53:19.346706Z",
     "start_time": "2018-11-17T01:53:10.161656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1106351   0.05385689 -0.01916586  0.7687107   0.22514213  5.53523435\n",
      " -0.00894829 -0.95596589  0.17579077 -0.01129689 -0.31605275  0.01521385\n",
      " -0.39603141]\n",
      "我们的训练误差 [[23.87429917]]\n",
      "我们的测试误差 [[27.8175622]]\n"
     ]
    }
   ],
   "source": [
    "# 自己实现轮子\n",
    "import numpy as np\n",
    "\n",
    "# 计算cost\n",
    "def cal_cost(theta, X, y):\n",
    "    y_hat = X.dot(theta)\n",
    "    residual = y_hat - y\n",
    "    result = residual.T.dot(residual)\n",
    "    return result/y.shape[0]\n",
    "\n",
    "def cal_grad(theta, X, y):\n",
    "    y_hat = np.dot(X, theta)\n",
    "    residual = y_hat - y\n",
    "    grad = (residual.T.dot(X)).T\n",
    "    return grad\n",
    "\n",
    "def my_linear_regression(X, y, alpha=0.03, num_iter=1000):\n",
    "    m, n = X_train.shape\n",
    "    theta = np.zeros((n,1))\n",
    "    y_tmp = y.reshape((m,1))\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        grad = cal_grad(theta, X, y_tmp)\n",
    "        theta = theta - alpha*grad\n",
    "\n",
    "    return theta\n",
    "\n",
    "theta = my_linear_regression(X_train, y_train, alpha = 0.00000001, num_iter=1000000)\n",
    "print(np.hstack(theta))\n",
    "print(\"我们的训练误差\", cal_cost(theta, X_train, y_train.reshape(-1,1)))\n",
    "print(\"我们的测试误差\", cal_cost(theta, X_test, y_test.reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用矩阵求导等于0的方法来求最优化的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T01:53:25.593750Z",
     "start_time": "2018-11-17T01:53:25.577934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.10765981  0.04839376 -0.02320055  2.99304063 -2.16262312  5.95900965\n",
      " -0.01692421 -1.02725147  0.16689425 -0.0104551  -0.3752965   0.01426569\n",
      " -0.34632603]\n",
      "我们的训练误差 [[23.4811376]]\n",
      "我们的测试误差 [[27.67810516]]\n",
      "sklearn的训练误差 [[23.4811376]]\n",
      "sklearn的测试误差 [[27.67810516]]\n"
     ]
    }
   ],
   "source": [
    "def my_linear_regression_matrix_method(X, y):\n",
    "    m, n =X.shape\n",
    "    y_tmp = y.reshape((m,1))\n",
    "    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y_tmp)\n",
    "    return theta\n",
    "\n",
    "theta = my_linear_regression_matrix_method(X_train, y_train)\n",
    "print(np.hstack(theta))\n",
    "print(\"我们的训练误差\", cal_cost(theta, X_train, y_train.reshape(-1,1)))\n",
    "print(\"我们的测试误差\", cal_cost(theta, X_test, y_test.reshape(-1,1)))\n",
    "print(\"sklearn的训练误差\", cal_cost(LR.coef_.reshape(-1,1), X_train, y_train.reshape((-1,1))))\n",
    "print(\"sklearn的测试误差\", cal_cost(LR.coef_.reshape(-1,1), X_test, y_test.reshape((-1,1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进阶\n",
    "从上面运行时间来看用矩阵运算的方法得到最优解似乎是最有效率的方法$\\Theta = (X^TX)^{-1}X^Ty$.\n",
    "但是,注意这里有一个求逆矩阵的操作.\n",
    "当数据量很大的时候(比如m>1000000), 对一个$m \\times m$维的矩阵求逆会需要很大的计算量, 这时用梯度下降等迭代方法求最优值会变得更加有效.\n",
    "比如:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T01:54:05.631978Z",
     "start_time": "2018-11-17T01:53:52.611178Z"
    }
   },
   "outputs": [],
   "source": [
    "A = np.random.rand(10000, 10000)\n",
    "B = np.linalg.inv(A)\n",
    "# executed in 45.8s, finished 14:14:43 2018-11-13\n",
    "# 10000 * 10000的矩阵求逆都要这么久, 而在机器学习中,样本数据通常是巨大的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以必须要想个办法优化此类操作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 附录\n",
    "## 为什么选择平方损失函数作为linear regression的损失函数\n",
    "### 从概率层面解释为什么将平方损失函数作为代价函数\n",
    "#### 中心极限定理\n",
    "当样本量N逐渐趋于无穷大时，N个抽样样本的均值的频数逐渐趋于正态分布，其对原总体的分布不做任何要求，意味着无论总体是什么分布，其抽样样本的均值的频数的分布都随着抽样数的增多而趋于正态分布。\n",
    "\n",
    "对于m个训练集样本，假设样本真实值与预测值之间的差值为$\\epsilon$, 即：\n",
    "$$\n",
    "y^{(i)} = \\hat{y}^{(i)} + \\epsilon^{(i)}\n",
    "$$\n",
    "由中心极限定理可知 $\\epsilon$有如下性质：\n",
    "1. $\\epsilon$是独立同分布的， 即， $x^{(i)}$ 与$x^{(j)}$之间互不影响，并且分布相同（使用的是同一款产品，或做同一件事，总之分布一样）\n",
    "2. $\\epsilon$服从均值为0，方差为$\\sigma^2$的高斯分布\n",
    "\n",
    "#### 极大似然估计\n",
    "利用已知的样本，反推最有可能导致这些样本的模型参数值；当然服从的模型也要是已知的（模型参数是未知的，就是要求模型参数）\n",
    "这里$\\epsilon$服从正太分布，所以服从的模型是已知的。符合应用极大似然估计的条件。\n",
    "\n",
    "$$\n",
    "p(\\epsilon) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left( -\\frac{(\\epsilon - 0)^2}{2\\sigma^2} \\right) \\\\\n",
    "= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left( -\\frac{\\epsilon^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "##### 似然函数\n",
    "$$\n",
    "L(\\theta) = \\prod_{i=1}^m p(\\epsilon^{(i)}) = \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left( -\\frac{(\\epsilon^{(i)})^2}{2\\sigma^2} \\right)\n",
    "\\\\ =  \\prod_{i=1}^m  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left( -\\frac{(y^{(i)} - \\hat{y}^{(i)})^2}{2\\sigma^2} \\right) \\\\\n",
    "= \\prod_{i=1}^m  \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left( -\\frac{(y^{(i)} - X^{(i)}\\theta)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "##### 对数似然函数\n",
    "由于$log(A\\cdot B) = logA + logB; log(exp(A)) = A $,因此\n",
    "$$\n",
    "l(\\theta) = log(L(\\theta)) = \\sum_{i=1}^m log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left( -\\frac{(y^{(i)} - X^{(i)}\\theta)^2}{2\\sigma^2} \\right)\\right) \\\\\n",
    "= \\sum_{i=1}^m log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\sum_{i=1}^m log \\left( exp\\left( -\\frac{(y^{(i)} - X^{(i)}\\theta)^2}{2\\sigma^2} \\right)\\right) \\\\\n",
    "= \\sum_{i=1}^m log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\sum_{i=1}^m \\left( -\\frac{(y^{(i)} - X^{(i)}\\theta)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "##### 极大化似然函数\n",
    "$$\n",
    "\\theta^* = arg\\underset{\\theta}{max}l(\\theta) = arg\\underset{\\theta}{max}\\left( \\sum_{i=1}^m log \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\sum_{i=1}^m \\left( -\\frac{(y^{(i)} - X^{(i)}\\theta)^2}{2\\sigma^2} \\right) \\right)\n",
    "$$\n",
    "在上式中自变量仅为$\\theta$，其他可以认为是常量， 因此：\n",
    "$$\n",
    "\\theta^* = arg\\underset{\\theta}{max}\\sum_{i=1}^m \\left( -(y^{(i)} - X^{(i)}\\theta)^2\\right)\n",
    "$$\n",
    "等价于\n",
    "$$\n",
    "\\theta^* = arg\\underset{\\theta}{min}\\sum_{i=1}^m \\left( (y^{(i)} - X^{(i)}\\theta)^2\\right)\n",
    "$$\n",
    "即最小化平方损失函数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
